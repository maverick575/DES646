{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6bUM-rnMUA4",
        "outputId": "fdee902f-8f70-4ba0-9395-2b1652db158b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install and import all required libraries\n",
        "!pip install -q selenium beautifulsoup4 pandas scikit-learn nltk requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Package Installation ---\n",
        "try:\n",
        "    import pandas\n",
        "    import nltk\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    import cloudscraper\n",
        "except ImportError:\n",
        "    print(\"Installing required packages...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                           \"pandas\", \"nltk\", \"vaderSentiment\", \"requests\",\n",
        "                           \"beautifulsoup4\", \"lxml\", \"cloudscraper\"])\n",
        "    print(\"Packages installed successfully.\")\n",
        "    import pandas\n",
        "    import nltk\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    import cloudscraper\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 1: ADVANCED MULTI-METHOD SCRAPER\n",
        "# ============================================================================\n",
        "\n",
        "class ProductScraper:\n",
        "    \"\"\"\n",
        "    Advanced scraper with multiple bypass methods:\n",
        "    1. Cloudscraper (bypasses basic Cloudflare protection)\n",
        "    2. Enhanced headers with cookies\n",
        "    3. Mobile user agent fallback\n",
        "    4. API endpoint scraping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Method 1: CloudScraper (best for Cloudflare)\n",
        "        self.scraper = cloudscraper.create_scraper(\n",
        "            browser={\n",
        "                'browser': 'chrome',\n",
        "                'platform': 'windows',\n",
        "                'mobile': False\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Method 2: Regular requests with enhanced headers\n",
        "        self.session = requests.Session()\n",
        "\n",
        "        self.desktop_agents = [\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0'\n",
        "        ]\n",
        "\n",
        "        self.mobile_agents = [\n",
        "            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',\n",
        "            'Mozilla/5.0 (Linux; Android 13; SM-S911B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36'\n",
        "        ]\n",
        "\n",
        "    def _get_headers(self, mobile=False):\n",
        "        \"\"\"Generate realistic headers\"\"\"\n",
        "        agents = self.mobile_agents if mobile else self.desktop_agents\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': random.choice(agents),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.9',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "            'Sec-Fetch-Dest': 'document',\n",
        "            'Sec-Fetch-Mode': 'navigate',\n",
        "            'Sec-Fetch-Site': 'none',\n",
        "            'Sec-Fetch-User': '?1',\n",
        "            'Cache-Control': 'max-age=0',\n",
        "            'TE': 'trailers'\n",
        "        }\n",
        "\n",
        "        # Add cookies to appear more legitimate\n",
        "        if not mobile:\n",
        "            headers['Cookie'] = 'session-id=262-1234567-1234567; ubid-acbin=262-1234567-1234567'\n",
        "\n",
        "        return headers\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        return \" \".join(text.split()).strip()\n",
        "\n",
        "    def _extract_product_name(self, soup) -> str:\n",
        "        \"\"\"Extract product name with multiple fallbacks\"\"\"\n",
        "        selectors = [\n",
        "            '#productTitle',\n",
        "            'h1#title',\n",
        "            'h1.product-title',\n",
        "            'span#productTitle',\n",
        "            'h1[class*=\"product\"]'\n",
        "        ]\n",
        "\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                elem = soup.select_one(selector)\n",
        "                if elem:\n",
        "                    text = self._clean_text(elem.get_text())\n",
        "                    if text and len(text) > 5:\n",
        "                        return text\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Last resort: find any h1 with product-like text\n",
        "        try:\n",
        "            all_h1 = soup.find_all('h1')\n",
        "            for h1 in all_h1:\n",
        "                text = self._clean_text(h1.get_text())\n",
        "                if text and len(text) > 10:\n",
        "                    return text\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return \"Unknown Product\"\n",
        "\n",
        "    def _extract_price(self, soup) -> float:\n",
        "        \"\"\"Extract price with multiple methods\"\"\"\n",
        "        # Method 1: Standard price selectors\n",
        "        price_selectors = [\n",
        "            'span.a-price-whole',\n",
        "            'span.a-price span.a-offscreen',\n",
        "            '#priceblock_ourprice',\n",
        "            '#priceblock_dealprice',\n",
        "            'span.a-color-price',\n",
        "            '.a-price .a-offscreen'\n",
        "        ]\n",
        "\n",
        "        for selector in price_selectors:\n",
        "            try:\n",
        "                elems = soup.select(selector)\n",
        "                for elem in elems:\n",
        "                    text = elem.get_text() if hasattr(elem, 'get_text') else elem.text\n",
        "                    text = re.sub(r'[‚Çπ$,\\s]', '', text)\n",
        "                    match = re.search(r'(\\d+\\.?\\d*)', text)\n",
        "                    if match:\n",
        "                        price = float(match.group(1))\n",
        "                        if 100 < price < 1000000:  # Sanity check\n",
        "                            return price\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Method 2: Search for price in scripts (JSON-LD)\n",
        "        try:\n",
        "            scripts = soup.find_all('script', type='application/ld+json')\n",
        "            for script in scripts:\n",
        "                try:\n",
        "                    data = json.loads(script.string)\n",
        "                    if 'offers' in data:\n",
        "                        price = float(data['offers'].get('price', 0))\n",
        "                        if price > 0:\n",
        "                            return price\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def _extract_reviews_and_ratings(self, soup) -> Tuple[List[str], List[float]]:\n",
        "        \"\"\"Extract reviews with multiple strategies\"\"\"\n",
        "        reviews = []\n",
        "        ratings = []\n",
        "\n",
        "        # Strategy 1: Standard review divs\n",
        "        review_containers = [\n",
        "            '[data-hook=\"review\"]',\n",
        "            '.review',\n",
        "            '[id*=\"customer-review\"]',\n",
        "            '.a-section.review'\n",
        "        ]\n",
        "\n",
        "        for container_selector in review_containers:\n",
        "            review_elements = soup.select(container_selector)\n",
        "\n",
        "            for review_elem in review_elements:\n",
        "                try:\n",
        "                    # Extract review text\n",
        "                    text_selectors = [\n",
        "                        '[data-hook=\"review-body\"]',\n",
        "                        '.review-text',\n",
        "                        '.review-text-content',\n",
        "                        '[class*=\"review-text\"]'\n",
        "                    ]\n",
        "\n",
        "                    review_text = None\n",
        "                    for text_sel in text_selectors:\n",
        "                        text_elem = review_elem.select_one(text_sel)\n",
        "                        if text_elem:\n",
        "                            review_text = self._clean_text(text_elem.get_text())\n",
        "                            break\n",
        "\n",
        "                    if not review_text or len(review_text) < 20:\n",
        "                        continue\n",
        "\n",
        "                    # Extract rating\n",
        "                    rating_selectors = [\n",
        "                        '[data-hook=\"review-star-rating\"]',\n",
        "                        '.review-rating',\n",
        "                        'i[class*=\"star\"]',\n",
        "                        '[class*=\"star-rating\"]'\n",
        "                    ]\n",
        "\n",
        "                    rating = 3.0  # default\n",
        "                    for rating_sel in rating_selectors:\n",
        "                        rating_elem = review_elem.select_one(rating_sel)\n",
        "                        if rating_elem:\n",
        "                            rating_text = rating_elem.get_text() if hasattr(rating_elem, 'get_text') else str(rating_elem)\n",
        "\n",
        "                            # Try different rating formats\n",
        "                            patterns = [\n",
        "                                r'(\\d+\\.?\\d*)\\s*(?:out\\s*of|stars?)',\n",
        "                                r'(\\d+\\.?\\d*)\\s*stars?',\n",
        "                                r'star-(\\d+)',\n",
        "                            ]\n",
        "\n",
        "                            for pattern in patterns:\n",
        "                                match = re.search(pattern, rating_text, re.IGNORECASE)\n",
        "                                if match:\n",
        "                                    rating = float(match.group(1))\n",
        "                                    break\n",
        "                            break\n",
        "\n",
        "                    reviews.append(review_text)\n",
        "                    ratings.append(rating)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            if reviews:  # If we found reviews, don't try other selectors\n",
        "                break\n",
        "\n",
        "        return reviews, ratings\n",
        "\n",
        "    def _try_mobile_version(self, url: str) -> Dict:\n",
        "        \"\"\"Try mobile Amazon site (often less protected)\"\"\"\n",
        "        mobile_url = url.replace('www.amazon.in', 'm.amazon.in')\n",
        "        print(f\"  - Trying mobile version...\")\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(mobile_url, headers=self._get_headers(mobile=True), timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "                product_name = self._extract_product_name(soup)\n",
        "                price = self._extract_price(soup)\n",
        "                reviews, ratings = self._extract_reviews_and_ratings(soup)\n",
        "\n",
        "                if reviews:\n",
        "                    return {\n",
        "                        'product_name': product_name,\n",
        "                        'price': price,\n",
        "                        'reviews': reviews,\n",
        "                        'ratings': ratings\n",
        "                    }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _try_reviews_page(self, url: str) -> Tuple[List[str], List[float]]:\n",
        "        \"\"\"Try dedicated reviews page\"\"\"\n",
        "        try:\n",
        "            # Extract ASIN\n",
        "            asin_match = re.search(r'/dp/([A-Z0-9]{10})', url)\n",
        "            if not asin_match:\n",
        "                return [], []\n",
        "\n",
        "            asin = asin_match.group(1)\n",
        "\n",
        "            # Try different review URL formats\n",
        "            review_urls = [\n",
        "                f\"https://www.amazon.in/product-reviews/{asin}/\",\n",
        "                f\"https://www.amazon.in/{asin}/product-reviews/\",\n",
        "                f\"https://m.amazon.in/product-reviews/{asin}/\"\n",
        "            ]\n",
        "\n",
        "            for review_url in review_urls:\n",
        "                try:\n",
        "                    print(f\"  - Trying reviews page: {review_url[:50]}...\")\n",
        "                    time.sleep(random.uniform(1, 2))\n",
        "\n",
        "                    # Try with cloudscraper first\n",
        "                    response = self.scraper.get(review_url, timeout=15)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "                        # Check if blocked\n",
        "                        if 'captcha' not in response.text.lower():\n",
        "                            reviews, ratings = self._extract_reviews_and_ratings(soup)\n",
        "                            if reviews:\n",
        "                                return reviews, ratings\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return [], []\n",
        "\n",
        "    def scrape_product(self, url: str) -> Dict:\n",
        "        \"\"\"Main scraping method with multiple fallback strategies\"\"\"\n",
        "        print(f\"‚ñ∂ Attempting to scrape: {url[:60]}...\")\n",
        "\n",
        "        methods = [\n",
        "            ('Cloudscraper (Desktop)', lambda: self._scrape_with_cloudscraper(url, False)),\n",
        "            ('Enhanced Requests', lambda: self._scrape_with_requests(url)),\n",
        "            ('Mobile Version', lambda: self._try_mobile_version(url)),\n",
        "            ('Reviews Page Direct', lambda: self._scrape_reviews_only(url))\n",
        "        ]\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            try:\n",
        "                print(f\"  - Method: {method_name}\")\n",
        "                result = method_func()\n",
        "\n",
        "                if result and result.get('reviews'):\n",
        "                    print(f\"  - ‚úì Success with {method_name}: {len(result['reviews'])} reviews\")\n",
        "                    return result\n",
        "\n",
        "                time.sleep(random.uniform(2, 4))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  - {method_name} failed: {str(e)[:50]}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"  - ‚õî All methods failed for this URL\")\n",
        "        return None\n",
        "\n",
        "    def _scrape_with_cloudscraper(self, url: str, mobile: bool = False) -> Dict:\n",
        "        \"\"\"Scrape using cloudscraper\"\"\"\n",
        "        response = self.scraper.get(url, timeout=20)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        # Check if blocked\n",
        "        page_text = soup.get_text().lower()\n",
        "        if 'captcha' in page_text or 'robot' in page_text[:500]:\n",
        "            return None\n",
        "\n",
        "        product_name = self._extract_product_name(soup)\n",
        "        price = self._extract_price(soup)\n",
        "        reviews, ratings = self._extract_reviews_and_ratings(soup)\n",
        "\n",
        "        # If no reviews on main page, try reviews page\n",
        "        if not reviews:\n",
        "            reviews, ratings = self._try_reviews_page(url)\n",
        "\n",
        "        if reviews:\n",
        "            return {\n",
        "                'product_name': product_name,\n",
        "                'price': price,\n",
        "                'reviews': reviews,\n",
        "                'ratings': ratings\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _scrape_with_requests(self, url: str) -> Dict:\n",
        "        \"\"\"Scrape using enhanced requests\"\"\"\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        response = self.session.get(url, headers=self._get_headers(), timeout=15)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        if 'captcha' in response.text.lower():\n",
        "            return None\n",
        "\n",
        "        product_name = self._extract_product_name(soup)\n",
        "        price = self._extract_price(soup)\n",
        "        reviews, ratings = self._extract_reviews_and_ratings(soup)\n",
        "\n",
        "        if not reviews:\n",
        "            reviews, ratings = self._try_reviews_page(url)\n",
        "\n",
        "        if reviews:\n",
        "            return {\n",
        "                'product_name': product_name,\n",
        "                'price': price,\n",
        "                'reviews': reviews,\n",
        "                'ratings': ratings\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _scrape_reviews_only(self, url: str) -> Dict:\n",
        "        \"\"\"Focus on getting reviews only\"\"\"\n",
        "        reviews, ratings = self._try_reviews_page(url)\n",
        "\n",
        "        if reviews:\n",
        "            # Get basic product info from main page if possible\n",
        "            try:\n",
        "                response = self.scraper.get(url, timeout=10)\n",
        "                soup = BeautifulSoup(response.content, 'lxml')\n",
        "                product_name = self._extract_product_name(soup)\n",
        "                price = self._extract_price(soup)\n",
        "            except:\n",
        "                product_name = \"Product\"\n",
        "                price = 0.0\n",
        "\n",
        "            return {\n",
        "                'product_name': product_name,\n",
        "                'price': price,\n",
        "                'reviews': reviews,\n",
        "                'ratings': ratings\n",
        "            }\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CSV Creation\n",
        "# ============================================================================\n",
        "\n",
        "def create_csv_from_data(products_data: List[Dict], filepath: str):\n",
        "    \"\"\"Creates a CSV file from the product data\"\"\"\n",
        "    print(f\"\\n‚ñ∂ Creating CSV file at '{filepath}'...\")\n",
        "    rows = []\n",
        "    for product in products_data:\n",
        "        for review, rating in zip(product['reviews'], product['ratings']):\n",
        "            rows.append({\n",
        "                'product_name': product['product_name'],\n",
        "                'price': product['price'],\n",
        "                'rating': rating,\n",
        "                'review_text': review\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    try:\n",
        "        df.to_csv(filepath, index=False)\n",
        "        print(f\" ‚úì Successfully saved {len(df)} reviews to '{filepath}'\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error saving CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 2: SENTIMENT ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def get_compound_score(self, text: str) -> float:\n",
        "        if not text:\n",
        "            return 0.0\n",
        "        return self.sia.polarity_scores(str(text))['compound']\n",
        "\n",
        "    def predict(self, text: str) -> Dict:\n",
        "        if not text:\n",
        "            return {'sentiment': 'Neutral', 'confidence': 1.0, 'compound_score': 0.0}\n",
        "\n",
        "        scores = self.sia.polarity_scores(str(text))\n",
        "        compound_score = scores['compound']\n",
        "\n",
        "        if compound_score >= 0.05:\n",
        "            sentiment_label = 'Positive'\n",
        "            confidence = scores['pos']\n",
        "        elif compound_score <= -0.05:\n",
        "            sentiment_label = 'Negative'\n",
        "            confidence = scores['neg']\n",
        "        else:\n",
        "            sentiment_label = 'Neutral'\n",
        "            confidence = scores['neu']\n",
        "\n",
        "        return {\n",
        "            'sentiment': sentiment_label,\n",
        "            'confidence': float(confidence),\n",
        "            'compound_score': float(compound_score)\n",
        "        }\n",
        "\n",
        "    def evaluate_on_data(self, products_data: List[Dict]) -> pd.DataFrame:\n",
        "        print(\"\\n‚ñ∂ Applying VADER sentiment analysis...\")\n",
        "        all_reviews = []\n",
        "        all_ratings = []\n",
        "        all_sentiments = []\n",
        "        all_comp_scores = []\n",
        "        product_names = []\n",
        "\n",
        "        for product in products_data:\n",
        "            for review, rating in zip(product['reviews'], product['ratings']):\n",
        "                prediction = self.predict(review)\n",
        "                all_reviews.append(review)\n",
        "                all_ratings.append(rating)\n",
        "                all_sentiments.append(prediction['sentiment'])\n",
        "                all_comp_scores.append(prediction['compound_score'])\n",
        "                product_names.append(product['product_name'])\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'product': product_names,\n",
        "            'review': all_reviews,\n",
        "            'original_rating': all_ratings,\n",
        "            'vader_sentiment': all_sentiments,\n",
        "            'vader_compound_score': all_comp_scores\n",
        "        })\n",
        "\n",
        "        print(f\" ‚úì Processed {len(df)} total reviews.\")\n",
        "        if len(df) > 0:\n",
        "            print(\"\\n ‚úì Sentiment Distribution:\")\n",
        "            print(df['vader_sentiment'].value_counts())\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 3: ASPECT-BASED ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class AspectAnalyzer:\n",
        "    def __init__(self, sentiment_analyzer: SentimentAnalyzer):\n",
        "        self.analyzer = sentiment_analyzer\n",
        "        self.aspect_keywords = {\n",
        "            'battery': ['battery', 'charge', 'charging', 'power', 'backup', 'mah', 'drain'],\n",
        "            'camera': ['camera', 'photo', 'picture', 'lens', 'zoom', 'video', 'selfie'],\n",
        "            'screen': ['screen', 'display', 'brightness', 'amoled', 'refresh'],\n",
        "            'performance': ['performance', 'speed', 'fast', 'slow', 'lag', 'processor', 'ram'],\n",
        "            'design': ['design', 'look', 'build', 'weight', 'premium', 'slim'],\n",
        "            'price': ['price', 'cost', 'expensive', 'cheap', 'value', 'worth'],\n",
        "            'sound': ['sound', 'audio', 'speaker', 'volume', 'music'],\n",
        "            'software': ['software', 'os', 'ui', 'update', 'bloatware', 'apps']\n",
        "        }\n",
        "\n",
        "    def analyze_aspects(self, product_data: Dict) -> Dict[str, Dict]:\n",
        "        reviews = product_data.get('reviews', [])\n",
        "        if not reviews:\n",
        "            return {}\n",
        "\n",
        "        aspect_scores = defaultdict(list)\n",
        "        aspect_counts = defaultdict(int)\n",
        "\n",
        "        for review in reviews:\n",
        "            review_lower = str(review).lower()\n",
        "            compound_score = self.analyzer.get_compound_score(review)\n",
        "\n",
        "            for aspect, keywords in self.aspect_keywords.items():\n",
        "                if any(kw in review_lower for kw in keywords):\n",
        "                    aspect_scores[aspect].append(compound_score)\n",
        "                    aspect_counts[aspect] += 1\n",
        "\n",
        "        aspect_analysis = {}\n",
        "        for aspect, scores in aspect_scores.items():\n",
        "            if scores:\n",
        "                avg_score = np.mean(scores)\n",
        "\n",
        "                if avg_score >= 0.05:\n",
        "                    sentiment_label = 'Positive'\n",
        "                elif avg_score <= -0.05:\n",
        "                    sentiment_label = 'Negative'\n",
        "                else:\n",
        "                    sentiment_label = 'Neutral'\n",
        "\n",
        "                aspect_analysis[aspect] = {\n",
        "                    'avg_compound_score': float(avg_score),\n",
        "                    'sentiment': sentiment_label,\n",
        "                    'mention_count': aspect_counts[aspect],\n",
        "                }\n",
        "\n",
        "        return aspect_analysis\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 4: LLM RECOMMENDATIONS\n",
        "# ============================================================================\n",
        "\n",
        "class GeminiAnalyzer:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        api_key=\"\"      #ENTER YOUR GEMINI API KEY HERE : get your gemini api key from here : https://aistudio.google.com/app/api-keys\n",
        "        self.api_key = api_key or os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
        "        if self.api_key:\n",
        "            self.api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={self.api_key}\"\n",
        "\n",
        "    def get_recommendations(self, product_data: Dict, aspect_data: Dict) -> str:\n",
        "        if not self.api_key:\n",
        "            return \"‚ö† No Gemini API key provided. Set GEMINI_API_KEY environment variable.\"\n",
        "\n",
        "        print(\"\\n‚ñ∂ Generating AI recommendations...\")\n",
        "\n",
        "        try:\n",
        "            aspect_summary = []\n",
        "            if aspect_data:\n",
        "                sorted_aspects = sorted(aspect_data.items(), key=lambda x: x[1]['avg_compound_score'])\n",
        "                for aspect, data in sorted_aspects:\n",
        "                    aspect_summary.append(\n",
        "                        f\"  - {aspect.upper()}: {data['sentiment']} (Score: {data['avg_compound_score']:.3f}, {data['mention_count']} mentions)\"\n",
        "                    )\n",
        "\n",
        "            prompt = f\"\"\"You are a tech product expert. Analyze this smartphone and recommend 3 BETTER alternatives currently available in India.\n",
        "\n",
        "CURRENT PRODUCT:\n",
        "Name: {product_data['product_name'][:100]}\n",
        "Price: ‚Çπ{product_data['price']:,.0f}\n",
        "\n",
        "USER REVIEW SENTIMENT ANALYSIS:\n",
        "{chr(10).join(aspect_summary) if aspect_summary else 'Limited review data'}\n",
        "\n",
        "TASK:\n",
        "Find 3 similar smartphones that are BETTER alternatives. For each alternative, provide:\n",
        "1. Product name and current price in India (‚Çπ)\n",
        "2. 2-3 lines explaining WHY it's better (specifically address the weak points above, especially camera if negative)\n",
        "3. Key improvements over the current product\n",
        "\n",
        "Format your response clearly with product names as headers.\"\"\"\n",
        "\n",
        "            payload = {\n",
        "                \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "                \"generationConfig\": {\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"topK\": 40,\n",
        "                    \"topP\": 0.95,\n",
        "                    \"maxOutputTokens\": 2048,\n",
        "                },\n",
        "                \"tools\": [{\"google_search\": {}}]\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                self.api_url,\n",
        "                headers={'Content-Type': 'application/json'},\n",
        "                json=payload,\n",
        "                timeout=90\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            # Debug: Print the full response structure\n",
        "            print(f\"  - API Response Status: {response.status_code}\")\n",
        "\n",
        "            # Extract text from response\n",
        "            candidates = result.get('candidates', [])\n",
        "            if not candidates:\n",
        "                print(f\"  - ‚ö† No candidates in response\")\n",
        "                return \"‚ö† No recommendations generated (API returned empty response)\"\n",
        "\n",
        "            content = candidates[0].get('content', {})\n",
        "            parts = content.get('parts', [])\n",
        "\n",
        "            if not parts:\n",
        "                print(f\"  - ‚ö† No parts in content\")\n",
        "                return \"‚ö† No recommendations generated (empty content)\"\n",
        "\n",
        "            # Combine all text parts\n",
        "            full_text = \"\"\n",
        "            for part in parts:\n",
        "                if 'text' in part:\n",
        "                    full_text += part['text'] + \"\\n\"\n",
        "\n",
        "            if not full_text.strip():\n",
        "                print(f\"  - ‚ö† Empty text in response\")\n",
        "                # Print raw response for debugging\n",
        "                print(f\"  - Raw response: {json.dumps(result, indent=2)[:500]}\")\n",
        "                return \"‚ö† No recommendations generated (text extraction failed)\"\n",
        "\n",
        "            print(f\"  - ‚úì Generated {len(full_text)} characters of recommendations\")\n",
        "            return full_text.strip()\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            error_detail = e.response.text if hasattr(e, 'response') else str(e)\n",
        "            print(f\"  - ‚õî API Error: {error_detail[:200]}\")\n",
        "            return f\"‚ö† API Error: {error_detail[:200]}\"\n",
        "        except Exception as e:\n",
        "            print(f\"  - ‚õî Error: {str(e)}\")\n",
        "            return f\"‚ö† Error generating recommendations: {str(e)[:200]}\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ENHANCED AMAZON PRODUCT SCRAPER & ANALYZER\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    PRODUCT_URL = input(\"\\nEnter Amazon India product URL: \").strip()\n",
        "\n",
        "    if not PRODUCT_URL:\n",
        "        print(\"‚ùå No URL provided\")\n",
        "        return\n",
        "\n",
        "    # --- STAGE 1: Web Scraping ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STAGE 1: MULTI-METHOD WEB SCRAPING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    scraper = ProductScraper()\n",
        "    product_data = scraper.scrape_product(PRODUCT_URL)\n",
        "\n",
        "    if not product_data or not product_data.get('reviews'):\n",
        "        print(\"\\n‚ùå Failed to scrape reviews from all methods.\")\n",
        "        print(\"\\nPossible solutions:\")\n",
        "        print(\"1. Use a VPN or different IP address\")\n",
        "        print(\"2. Try a different product URL\")\n",
        "        print(\"3. Run the script from a cloud service (Google Colab, etc.)\")\n",
        "        print(\"4. Use Amazon Product Advertising API (requires approval)\")\n",
        "        return\n",
        "\n",
        "    products_list = [product_data]\n",
        "    df = create_csv_from_data(products_list, 'scraped_product_data.csv')\n",
        "\n",
        "    # --- STAGE 2: Sentiment Analysis ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STAGE 2: SENTIMENT ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    sentiment_analyzer = SentimentAnalyzer()\n",
        "    sentiment_df = sentiment_analyzer.evaluate_on_data(products_list)\n",
        "\n",
        "    # --- STAGE 3: Aspect Analysis ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STAGE 3: ASPECT-BASED ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    aspect_analyzer = AspectAnalyzer(sentiment_analyzer)\n",
        "    aspects = aspect_analyzer.analyze_aspects(product_data)\n",
        "\n",
        "    if aspects:\n",
        "        print(f\"\\n‚ñ∂ Analysis for: {product_data['product_name']}\")\n",
        "        print(f\"  {'Aspect':<14} | {'Mentions':<8} | {'Score':<8} | {'Sentiment'}\")\n",
        "        print(\"  \" + \"-\" * 50)\n",
        "\n",
        "        for aspect, data in sorted(aspects.items(), key=lambda x: x[1]['mention_count'], reverse=True):\n",
        "            emoji = 'üòä' if data['sentiment'] == 'Positive' else 'üòê' if data['sentiment'] == 'Neutral' else 'üòû'\n",
        "            print(f\"  {aspect.upper():<14} | {data['mention_count']:<8} | {data['avg_compound_score']:<8.3f} | {data['sentiment']} {emoji}\")\n",
        "\n",
        "    # --- STAGE 4: AI Recommendations ---\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STAGE 4: AI RECOMMENDATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    gemini = GeminiAnalyzer()\n",
        "    recommendations = gemini.get_recommendations(product_data, aspects)\n",
        "\n",
        "    print(\"\\n\" + \"‚îÄ\" * 80)\n",
        "    print(\"üì± ALTERNATIVE PRODUCT RECOMMENDATIONS:\")\n",
        "    print(\"‚îÄ\" * 80)\n",
        "    print(recommendations)\n",
        "    print(\"‚îÄ\" * 80)\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "    print(f\"   ‚Ä¢ Scraped: {len(product_data.get('reviews', []))} reviews\")\n",
        "    print(f\"   ‚Ä¢ Analyzed: {len(aspects)} product aspects\")\n",
        "    print(f\"   ‚Ä¢ CSV saved: scraped_product_data.csv\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP-gx6T-MfNI",
        "outputId": "2cc35ea6-3793-4a58-a82e-5779ec985701"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ENHANCED AMAZON PRODUCT SCRAPER & ANALYZER\n",
            "================================================================================\n",
            "\n",
            "Enter Amazon India product URL: https://www.amazon.in/Galaxy-S24-FE-Graphite-Processor/dp/B0F1YQDGL9/ref=pd_sbs_d_sccl_1_4/521-6229103-3245537?pd_rd_w=KM0q9&content-id=amzn1.sym.6d240404-f8ea-42f5-98fe-bf3c8ec77086&pf_rd_p=6d240404-f8ea-42f5-98fe-bf3c8ec77086&pf_rd_r=18FFJYGBX5BYN57AC59C&pd_rd_wg=wsTmc&pd_rd_r=09ff76bc-efc4-42a9-ba7a-a14c7682e430&pd_rd_i=B0F1YQDGL9&psc=1\n",
            "\n",
            "================================================================================\n",
            "STAGE 1: MULTI-METHOD WEB SCRAPING\n",
            "================================================================================\n",
            "‚ñ∂ Attempting to scrape: https://www.amazon.in/Galaxy-S24-FE-Graphite-Processor/dp/B0...\n",
            "  - Method: Cloudscraper (Desktop)\n",
            "  - ‚úì Success with Cloudscraper (Desktop): 8 reviews\n",
            "\n",
            "‚ñ∂ Creating CSV file at 'scraped_product_data.csv'...\n",
            " ‚úì Successfully saved 8 reviews to 'scraped_product_data.csv'\n",
            "\n",
            "================================================================================\n",
            "STAGE 2: SENTIMENT ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "‚ñ∂ Applying VADER sentiment analysis...\n",
            " ‚úì Processed 8 total reviews.\n",
            "\n",
            " ‚úì Sentiment Distribution:\n",
            "vader_sentiment\n",
            "Positive    7\n",
            "Negative    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "STAGE 3: ASPECT-BASED ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "‚ñ∂ Analysis for: Samsung Galaxy S24 FE 5G (Graphite, 8GB RAM, 256GB Storage) | Exynos 2400e Processor | Dynamic AMOLED Display | Dual Nano-SIM | 4700mAh Battery\n",
            "  Aspect         | Mentions | Score    | Sentiment\n",
            "  --------------------------------------------------\n",
            "  BATTERY        | 7        | 0.721    | Positive üòä\n",
            "  CAMERA         | 5        | 0.931    | Positive üòä\n",
            "  PERFORMANCE    | 4        | 0.552    | Positive üòä\n",
            "  SOFTWARE       | 4        | 0.627    | Positive üòä\n",
            "  DESIGN         | 3        | 0.870    | Positive üòä\n",
            "  PRICE          | 3        | 0.463    | Positive üòä\n",
            "  SCREEN         | 1        | 0.997    | Positive üòä\n",
            "\n",
            "================================================================================\n",
            "STAGE 4: AI RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "‚ñ∂ Generating AI recommendations...\n",
            "  - API Response Status: 200\n",
            "  - ‚úì Generated 2154 characters of recommendations\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üì± ALTERNATIVE PRODUCT RECOMMENDATIONS:\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Okay, I will analyze the Samsung Galaxy S24 FE 5G based on the provided information and recommend three better alternatives available in India, focusing on addressing potential weaknesses even though the review sentiment is generally positive.\n",
            "\n",
            "\n",
            "Here are three smartphones that could be considered better alternatives to the Samsung Galaxy S24 FE 5G in India, based on current information (November 5, 2025) and focusing on improvements and value within a similar price range:\n",
            "\n",
            "## 1. OnePlus 13R\n",
            "\n",
            "*   **Price:** ‚Çπ38,999 (approx.)\n",
            "*   **Why it's better:** The OnePlus 13R offers a potentially faster processor for gaming and demanding tasks. While the S24 FE has a good Exynos chip, the OnePlus 13R often focuses on raw performance. It also has very fast charging.\n",
            "*   **Key Improvements:**\n",
            "    *   Potentially faster Snapdragon processor for improved gaming and overall performance.\n",
            "    *   Faster charging technology (likely 80W or 100W) compared to the S24 FE's 25W.\n",
            "\n",
            "## 2. Realme GT 7\n",
            "\n",
            "*   **Price:** ‚Çπ35,990 (approx.)\n",
            "*   **Why it's better:** The Realme GT 7 stands out due to its exceptional battery life and rapid charging capabilities. It also offers a vibrant display.\n",
            "*   **Key Improvements:**\n",
            "    *   Significantly larger battery capacity (potentially 7000mAh) compared to the S24 FE, offering much longer battery life.\n",
            "    *   Faster charging (120W) for quicker top-ups.\n",
            "\n",
            "## 3. Vivo V40 Pro\n",
            "\n",
            "*   **Price:** ‚Çπ36,999 (approx.)\n",
            "*   **Why it's better:** The Vivo V40 Pro typically excels in camera performance, especially in portrait mode and low-light conditions. It often features a versatile camera system with multiple lenses.\n",
            "*   **Key Improvements:**\n",
            "    *   Potentially better camera system, especially for portraits and low-light photography. Vivo is known for its camera-centric smartphones.\n",
            "    *   Slim design with curved edge display.\n",
            "\n",
            "These recommendations are based on the assumption that the user values camera improvements and potentially better performance or battery life. Prices are approximate and may vary. It's always advisable to check the latest prices and specifications from reliable retailers before making a purchase.\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ANALYSIS COMPLETE!\n",
            "   ‚Ä¢ Scraped: 8 reviews\n",
            "   ‚Ä¢ Analyzed: 7 product aspects\n",
            "   ‚Ä¢ CSV saved: scraped_product_data.csv\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}